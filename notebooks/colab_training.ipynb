{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubeMaster: Color Classification Model Training\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mvipin/cubemaster/blob/main/notebooks/colab_training.ipynb)\n",
    "\n",
    "This notebook enables training of **MLP** and **Shallow CNN** models for Rubik's Cube color classification using Google Colab's GPU resources.\n",
    "\n",
    "## Contents\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [Data Preparation](#2-data-preparation)\n",
    "3. [Model Training](#3-model-training)\n",
    "4. [Weights & Biases Integration](#4-weights--biases-integration)\n",
    "   - 4.1 Authentication\n",
    "   - 4.2 Training with Logging\n",
    "   - 4.3 **Hyperparameter Sweeps** ‚ö°\n",
    "   - 4.4 Resume/Join Sweeps\n",
    "   - 4.5 Sweep Results Analysis\n",
    "5. [Results Visualization](#5-results-visualization)\n",
    "6. [Model Export](#6-model-export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slower.\")\n",
    "    print(\"   Go to Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision torchmetrics\n",
    "!pip install -q albumentations opencv-python-headless Pillow\n",
    "!pip install -q numpy pyyaml tqdm\n",
    "!pip install -q matplotlib seaborn scikit-learn\n",
    "!pip install -q onnx onnxruntime\n",
    "!pip install -q wandb\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Install Git LFS (required for dataset files)\n",
    "!apt-get install -qq git-lfs\n",
    "!git lfs install\n",
    "\n",
    "# Clone the repository\n",
    "REPO_URL = \"https://github.com/mvipin/cubemaster.git\"\n",
    "REPO_DIR = Path(\"/content/cubemaster\")\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "    print(f\"‚úÖ Repository cloned to {REPO_DIR}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Repository already exists at {REPO_DIR}\")\n",
    "    # Pull latest changes\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# Pull LFS files (dataset images)\n",
    "print(\"\\nüì• Pulling Git LFS files (dataset images)...\")\n",
    "!cd {REPO_DIR} && git lfs pull\n",
    "print(\"‚úÖ LFS files downloaded\")\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Setup Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to Python path\n",
    "SRC_PATH = Path(\"/content/cubemaster/src\")\n",
    "if str(SRC_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_PATH))\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from cubemaster.models import MLPClassifier, ShallowCNNClassifier, MODEL_REGISTRY\n",
    "    from cubemaster.utils.config import load_config, get_device, set_seed\n",
    "    from cubemaster.training.dataset import CubeColorDataset\n",
    "    from cubemaster.training.augmentations import get_train_transforms, get_val_transforms\n",
    "    from cubemaster.training.trainer import Trainer, EarlyStopping\n",
    "    from cubemaster import COLOR_CLASSES\n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "    print(f\"   Available models: {list(MODEL_REGISTRY.keys())}\")\n",
    "    print(f\"   Color classes: {COLOR_CLASSES}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"   Make sure the repository is cloned correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Preparation\n",
    "\n",
    "### 2.1 Dataset Selection\n",
    "\n",
    "The CubeMaster repository includes a pre-processed dataset with 4,410 images ready for training.\n",
    "\n",
    "**Dataset options:**\n",
    "1. **Repository Dataset** (default) - Use the included `data/processed/` dataset\n",
    "2. **Google Drive** - Upload your own dataset via Google Drive\n",
    "3. **Direct Upload** - Upload a zip file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Dataset source selection\n",
    "# Options: 'repository', 'google_drive', 'upload'\n",
    "DATASET_SOURCE = 'repository'  # Default: use included dataset\n",
    "\n",
    "if DATASET_SOURCE == 'repository':\n",
    "    # Use the dataset included in the cloned repository\n",
    "    DATASET_PATH = Path(\"/content/cubemaster/data/processed\")\n",
    "    \n",
    "    if DATASET_PATH.exists():\n",
    "        print(f\"‚úÖ Using repository dataset: {DATASET_PATH}\")\n",
    "        print(\"   This dataset contains 4,410 images (train: 4180, val: 101, test: 129)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Repository dataset not found at {DATASET_PATH}\")\n",
    "        print(\"   Make sure the repository was cloned correctly.\")\n",
    "\n",
    "elif DATASET_SOURCE == 'google_drive':\n",
    "    # Mount Google Drive and use custom dataset\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set path to your dataset in Google Drive\n",
    "    # Expected structure: dataset/train/{B,G,O,R,W,Y}/*.jpg\n",
    "    DATASET_PATH = Path(\"/content/drive/MyDrive/CubeMaster/dataset\")\n",
    "    print(f\"üìÅ Using Google Drive dataset: {DATASET_PATH}\")\n",
    "\n",
    "elif DATASET_SOURCE == 'upload':\n",
    "    # Direct upload via Colab file picker\n",
    "    from google.colab import files\n",
    "    import zipfile\n",
    "    \n",
    "    print(\"Upload your dataset as a zip file:\")\n",
    "    print(\"Expected structure: train/{B,G,O,R,W,Y}/*.jpg\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Extract uploaded zip\n",
    "    for filename in uploaded.keys():\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('/content/dataset')\n",
    "    DATASET_PATH = Path(\"/content/dataset\")\n",
    "    print(f\"‚úÖ Dataset extracted to {DATASET_PATH}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown DATASET_SOURCE: {DATASET_SOURCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Verify Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def verify_dataset(dataset_path: Path):\n",
    "    \"\"\"Verify dataset structure and count samples.\"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    if not dataset_path.exists():\n",
    "        print(f\"‚ùå Dataset path does not exist: {dataset_path}\")\n",
    "        return None\n",
    "    \n",
    "    splits = ['train', 'val', 'test']\n",
    "    expected_classes = ['B', 'G', 'O', 'R', 'W', 'Y']\n",
    "    stats = defaultdict(dict)\n",
    "    \n",
    "    print(f\"üìä Dataset Statistics for: {dataset_path}\\n\")\n",
    "    print(f\"{'Split':<10} {'B':>6} {'G':>6} {'O':>6} {'R':>6} {'W':>6} {'Y':>6} {'Total':>8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for split in splits:\n",
    "        split_path = dataset_path / split\n",
    "        if not split_path.exists():\n",
    "            print(f\"‚ö†Ô∏è  {split:<10} - Directory not found\")\n",
    "            continue\n",
    "        \n",
    "        total = 0\n",
    "        row = f\"{split:<10}\"\n",
    "        for cls in expected_classes:\n",
    "            cls_path = split_path / cls\n",
    "            if cls_path.exists():\n",
    "                count = len(list(cls_path.glob('*.png'))) + len(list(cls_path.glob('*.jpg')))\n",
    "                stats[split][cls] = count\n",
    "                total += count\n",
    "                row += f\" {count:>6}\"\n",
    "            else:\n",
    "                row += f\" {'N/A':>6}\"\n",
    "        stats[split]['total'] = total\n",
    "        row += f\" {total:>8}\"\n",
    "        print(row)\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    return stats\n",
    "\n",
    "# Verify dataset\n",
    "dataset_stats = verify_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Setup Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from cubemaster.training.dataset import CubeColorDataset\n",
    "from cubemaster.training.augmentations import get_train_transforms, get_val_transforms\n",
    "\n",
    "def create_data_loaders(dataset_path, batch_size=32, num_workers=2, image_size=(50, 50)):\n",
    "    \"\"\"Create train, validation, and test data loaders.\"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Get transforms\n",
    "    train_transform = get_train_transforms(image_size)\n",
    "    val_transform = get_val_transforms(image_size)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CubeColorDataset(\n",
    "        root_dir=dataset_path / 'train',\n",
    "        transform=train_transform\n",
    "    )\n",
    "    val_dataset = CubeColorDataset(\n",
    "        root_dir=dataset_path / 'val',\n",
    "        transform=val_transform\n",
    "    )\n",
    "    test_dataset = CubeColorDataset(\n",
    "        root_dir=dataset_path / 'test',\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data loaders created:\")\n",
    "    print(f\"   Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
    "    print(f\"   Val:   {len(val_dataset)} samples, {len(val_loader)} batches\")\n",
    "    print(f\"   Test:  {len(test_dataset)} samples, {len(test_loader)} batches\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Create data loaders (will be called after config is set)\n",
    "print(\"Data loader function defined. Will be created during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Training\n",
    "\n",
    "### 3.1 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from cubemaster.utils.config import load_config, set_seed\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    # Model selection: 'mlp' or 'shallow_cnn'\n",
    "    'model_type': 'mlp',  # Change to 'shallow_cnn' for CNN training\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'batch_size': 32,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    \n",
    "    # MLP-specific\n",
    "    'mlp_hidden_dims': [256, 128],\n",
    "    'mlp_dropout': 0.3,\n",
    "    \n",
    "    # Shallow CNN-specific\n",
    "    'cnn_dropout': 0.5,\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 10,\n",
    "    'min_delta': 0.001,\n",
    "    \n",
    "    # Reproducibility\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Wandb (optional)\n",
    "    'use_wandb': False,  # Set to True to enable wandb logging\n",
    "    'wandb_project': 'cubemaster',\n",
    "}\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(TRAINING_CONFIG['seed'])\n",
    "print(f\"‚úÖ Configuration set for {TRAINING_CONFIG['model_type'].upper()} training\")\n",
    "print(f\"   Epochs: {TRAINING_CONFIG['epochs']}, Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"   Learning rate: {TRAINING_CONFIG['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from cubemaster.models import MLPClassifier\n",
    "from cubemaster.training.trainer import Trainer, EarlyStopping\n",
    "from cubemaster.utils.config import get_device\n",
    "\n",
    "def train_mlp(config, train_loader, val_loader):\n",
    "    \"\"\"Train MLP model with given configuration.\"\"\"\n",
    "    device = get_device()\n",
    "    print(f\"\\nüöÄ Training MLP on {device}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create model\n",
    "    model = MLPClassifier(\n",
    "        num_classes=6,\n",
    "        input_size=(50, 50),\n",
    "        hidden_dims=config['mlp_hidden_dims'],\n",
    "        dropout_rate=config['mlp_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    params = model.count_parameters()\n",
    "    print(f\"Model parameters: {params['trainable']:,} trainable\")\n",
    "    \n",
    "    # Setup training components\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=config['patience'],\n",
    "        min_delta=config['min_delta']\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        early_stopping=early_stopping,\n",
    "        checkpoint_dir=Path('/content/cubemaster/models/mlp'),\n",
    "        use_wandb=config['use_wandb']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = trainer.fit(train_loader, val_loader, epochs=config['epochs'])\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Training complete! Best val accuracy: {trainer.best_val_acc:.2f}%\")\n",
    "    \n",
    "    return model, history, trainer\n",
    "\n",
    "# Train MLP (set model_type='mlp' in config above)\n",
    "if TRAINING_CONFIG['model_type'] == 'mlp':\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        DATASET_PATH, \n",
    "        batch_size=TRAINING_CONFIG['batch_size']\n",
    "    )\n",
    "    mlp_model, mlp_history, mlp_trainer = train_mlp(TRAINING_CONFIG, train_loader, val_loader)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Skipping MLP training. Set model_type='mlp' to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train Shallow CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cubemaster.models import ShallowCNNClassifier\n",
    "\n",
    "def train_shallow_cnn(config, train_loader, val_loader):\n",
    "    \"\"\"Train Shallow CNN model with given configuration.\"\"\"\n",
    "    device = get_device()\n",
    "    print(f\"\\nüöÄ Training Shallow CNN on {device}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create model\n",
    "    model = ShallowCNNClassifier(\n",
    "        num_classes=6,\n",
    "        input_size=(50, 50),\n",
    "        dropout_rate=config['cnn_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    params = model.count_parameters()\n",
    "    print(f\"Model parameters: {params['trainable']:,} trainable\")\n",
    "    \n",
    "    # Setup training components\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=config['patience'],\n",
    "        min_delta=config['min_delta']\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        early_stopping=early_stopping,\n",
    "        checkpoint_dir=Path('/content/cubemaster/models/shallow_cnn'),\n",
    "        use_wandb=config['use_wandb']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = trainer.fit(train_loader, val_loader, epochs=config['epochs'])\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Training complete! Best val accuracy: {trainer.best_val_acc:.2f}%\")\n",
    "    \n",
    "    return model, history, trainer\n",
    "\n",
    "# Train Shallow CNN (set model_type='shallow_cnn' in config above)\n",
    "if TRAINING_CONFIG['model_type'] == 'shallow_cnn':\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        DATASET_PATH, \n",
    "        batch_size=TRAINING_CONFIG['batch_size']\n",
    "    )\n",
    "    cnn_model, cnn_history, cnn_trainer = train_shallow_cnn(TRAINING_CONFIG, train_loader, val_loader)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Skipping Shallow CNN training. Set model_type='shallow_cnn' to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Weights & Biases Integration\n",
    "\n",
    "### 4.1 Wandb Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb setup for Colab\n",
    "import wandb\n",
    "\n",
    "# Login to wandb (will prompt for API key)\n",
    "# Get your API key from: https://wandb.ai/authorize\n",
    "wandb.login()\n",
    "\n",
    "print(\"‚úÖ Logged into Weights & Biases!\")\n",
    "print(\"   Dashboard: https://wandb.ai/home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train with Wandb Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable wandb and retrain\n",
    "TRAINING_CONFIG['use_wandb'] = True\n",
    "\n",
    "# Initialize wandb run\n",
    "run = wandb.init(\n",
    "    project=TRAINING_CONFIG['wandb_project'],\n",
    "    name=f\"{TRAINING_CONFIG['model_type']}_colab_{TRAINING_CONFIG['seed']}\",\n",
    "    config=TRAINING_CONFIG,\n",
    "    tags=['colab', TRAINING_CONFIG['model_type']]\n",
    ")\n",
    "\n",
    "print(f\"üìä Wandb run initialized: {run.url}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    DATASET_PATH, \n",
    "    batch_size=TRAINING_CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "# Train selected model with wandb\n",
    "if TRAINING_CONFIG['model_type'] == 'mlp':\n",
    "    model, history, trainer = train_mlp(TRAINING_CONFIG, train_loader, val_loader)\n",
    "else:\n",
    "    model, history, trainer = train_shallow_cnn(TRAINING_CONFIG, train_loader, val_loader)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "print(f\"\\n‚úÖ Training logged to wandb: {run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Hyperparameter Sweeps\n",
    "\n",
    "Run automated hyperparameter optimization using wandb sweeps. This section allows you to:\n",
    "- Create new sweeps with predefined configurations\n",
    "- Run sweep agents to explore hyperparameter space\n",
    "- Monitor sweep progress in real-time\n",
    "- Resume or join existing sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SWEEP CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Choose which model to sweep\n",
    "SWEEP_MODEL = 'shallow_cnn'  # Options: 'mlp', 'shallow_cnn'\n",
    "\n",
    "# Sweep settings\n",
    "SWEEP_CONFIG = {\n",
    "    'wandb_project': 'cubemaster',\n",
    "    'wandb_entity': None,  # Set to your wandb username/team if needed\n",
    "    'max_runs': 20,  # Maximum runs per sweep (Colab-friendly limit)\n",
    "    'epochs_per_run': 15,  # Epochs per training run\n",
    "}\n",
    "\n",
    "print(f\"üîß Sweep Configuration:\")\n",
    "print(f\"   Model: {SWEEP_MODEL}\")\n",
    "print(f\"   Max runs: {SWEEP_CONFIG['max_runs']}\")\n",
    "print(f\"   Epochs per run: {SWEEP_CONFIG['epochs_per_run']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SWEEP DEFINITIONS\n",
    "# ============================================================\n",
    "\n",
    "# MLP Sweep Configuration\n",
    "MLP_SWEEP_CONFIG = {\n",
    "    'method': 'bayes',  # bayes, random, or grid\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 0.00001,\n",
    "            'max': 0.01\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64, 128]\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.1,\n",
    "            'max': 0.5\n",
    "        },\n",
    "        'hidden_dims': {\n",
    "            'values': [\n",
    "                [128, 64],\n",
    "                [256, 128],\n",
    "                [512, 256],\n",
    "                [256, 128, 64],\n",
    "                [512, 256, 128]\n",
    "            ]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 0.00001,\n",
    "            'max': 0.01\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'adamw']\n",
    "        },\n",
    "        'label_smoothing': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0,\n",
    "            'max': 0.2\n",
    "        }\n",
    "    },\n",
    "    'early_terminate': {\n",
    "        'type': 'hyperband',\n",
    "        'min_iter': 5,\n",
    "        'eta': 3,\n",
    "        's': 2\n",
    "    }\n",
    "}\n",
    "\n",
    "# Shallow CNN Sweep Configuration\n",
    "SHALLOW_CNN_SWEEP_CONFIG = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 0.0001,\n",
    "            'max': 0.01\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64, 128]\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.2,\n",
    "            'max': 0.6\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 0.00001,\n",
    "            'max': 0.01\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'adamw', 'sgd']\n",
    "        },\n",
    "        'label_smoothing': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0,\n",
    "            'max': 0.2\n",
    "        },\n",
    "        'rotation_limit': {\n",
    "            'values': [10, 15, 20, 30]\n",
    "        },\n",
    "        'brightness_limit': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.1,\n",
    "            'max': 0.3\n",
    "        }\n",
    "    },\n",
    "    'early_terminate': {\n",
    "        'type': 'hyperband',\n",
    "        'min_iter': 5,\n",
    "        'eta': 3,\n",
    "        's': 2\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select sweep config based on model choice\n",
    "ACTIVE_SWEEP_CONFIG = MLP_SWEEP_CONFIG if SWEEP_MODEL == 'mlp' else SHALLOW_CNN_SWEEP_CONFIG\n",
    "print(f\"‚úÖ Loaded {SWEEP_MODEL.upper()} sweep configuration\")\n",
    "print(f\"   Method: {ACTIVE_SWEEP_CONFIG['method']}\")\n",
    "print(f\"   Parameters: {list(ACTIVE_SWEEP_CONFIG['parameters'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SWEEP TRAINING FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "from cubemaster.models import MLPClassifier, ShallowCNNClassifier\n",
    "from cubemaster.training.trainer import Trainer, EarlyStopping\n",
    "from cubemaster.training.dataset import CubeColorDataset\n",
    "from cubemaster.training.augmentations import get_train_transforms, get_val_transforms\n",
    "from cubemaster.utils.config import get_device\n",
    "\n",
    "def sweep_train():\n",
    "    \"\"\"Training function called by wandb sweep agent.\"\"\"\n",
    "    # Initialize wandb run (sweep agent handles config)\n",
    "    run = wandb.init()\n",
    "    config = wandb.config\n",
    "    \n",
    "    device = get_device()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Sweep Run: {run.name}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Config: {dict(config)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Get hyperparameters from sweep config\n",
    "        lr = config.get('lr', 0.001)\n",
    "        batch_size = config.get('batch_size', 32)\n",
    "        dropout_rate = config.get('dropout_rate', 0.3)\n",
    "        weight_decay = config.get('weight_decay', 0.0001)\n",
    "        optimizer_name = config.get('optimizer', 'adam')\n",
    "        label_smoothing = config.get('label_smoothing', 0.1)\n",
    "        \n",
    "        # Augmentation params\n",
    "        rotation_limit = config.get('rotation_limit', 15)\n",
    "        brightness_limit = config.get('brightness_limit', 0.2)\n",
    "        \n",
    "        # Image size\n",
    "        image_size = (50, 50)\n",
    "        \n",
    "        # Create transforms with sweep augmentation params\n",
    "        train_aug_config = {\n",
    "            'rotation_limit': rotation_limit,\n",
    "            'brightness_limit': brightness_limit,\n",
    "            'horizontal_flip': True,\n",
    "            'normalize': True\n",
    "        }\n",
    "        train_transform = get_train_transforms(image_size, train_aug_config)\n",
    "        val_transform = get_val_transforms(image_size)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = CubeColorDataset(\n",
    "            root_dir=DATASET_PATH / 'train',\n",
    "            transform=train_transform\n",
    "        )\n",
    "        val_dataset = CubeColorDataset(\n",
    "            root_dir=DATASET_PATH / 'val',\n",
    "            transform=val_transform\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True,\n",
    "            num_workers=2, pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=batch_size, shuffle=False,\n",
    "            num_workers=2, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Create model based on sweep type\n",
    "        if SWEEP_MODEL == 'mlp':\n",
    "            hidden_dims = config.get('hidden_dims', [256, 128])\n",
    "            model = MLPClassifier(\n",
    "                num_classes=6,\n",
    "                input_size=image_size,\n",
    "                hidden_dims=hidden_dims,\n",
    "                dropout_rate=dropout_rate\n",
    "            ).to(device)\n",
    "        else:\n",
    "            model = ShallowCNNClassifier(\n",
    "                num_classes=6,\n",
    "                input_size=image_size,\n",
    "                dropout_rate=dropout_rate\n",
    "            ).to(device)\n",
    "        \n",
    "        params = model.count_parameters()\n",
    "        print(f\"Model: {SWEEP_MODEL.upper()}, Parameters: {params['trainable']:,}\")\n",
    "        \n",
    "        # Create optimizer\n",
    "        if optimizer_name == 'adam':\n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.parameters(), lr=lr, weight_decay=weight_decay\n",
    "            )\n",
    "        elif optimizer_name == 'adamw':\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                model.parameters(), lr=lr, weight_decay=weight_decay\n",
    "            )\n",
    "        else:  # sgd\n",
    "            optimizer = torch.optim.SGD(\n",
    "                model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9\n",
    "            )\n",
    "        \n",
    "        # Scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=SWEEP_CONFIG['epochs_per_run'], eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Loss with label smoothing\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "        \n",
    "        # Trainer (wandb logging handled by run context)\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            early_stopping=early_stopping,\n",
    "            checkpoint_dir=Path(f'/content/sweep_checkpoints/{run.name}'),\n",
    "            use_wandb=True\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        history = trainer.fit(\n",
    "            train_loader, val_loader, \n",
    "            epochs=SWEEP_CONFIG['epochs_per_run']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Run complete! Best val accuracy: {trainer.best_val_acc:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Run failed: {e}\")\n",
    "        wandb.log({'error': str(e)})\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        # Clean up GPU memory\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Sweep training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE AND RUN SWEEP\n",
    "# ============================================================\n",
    "\n",
    "# Create a new sweep\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep=ACTIVE_SWEEP_CONFIG,\n",
    "    project=SWEEP_CONFIG['wandb_project'],\n",
    "    entity=SWEEP_CONFIG['wandb_entity']\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Sweep created!\")\n",
    "print(f\"   Sweep ID: {sweep_id}\")\n",
    "print(f\"   Dashboard: https://wandb.ai/{SWEEP_CONFIG['wandb_entity'] or 'your-entity'}/{SWEEP_CONFIG['wandb_project']}/sweeps/{sweep_id}\")\n",
    "print(f\"\\n   To run this sweep later from CLI:\")\n",
    "print(f\"   wandb agent {SWEEP_CONFIG['wandb_project']}/{sweep_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN SWEEP AGENT\n",
    "# ============================================================\n",
    "\n",
    "# Run the sweep agent\n",
    "# This will train models with different hyperparameters\n",
    "print(f\"üèÉ Starting sweep agent...\")\n",
    "print(f\"   Max runs: {SWEEP_CONFIG['max_runs']}\")\n",
    "print(f\"   Model: {SWEEP_MODEL}\")\n",
    "print(f\"\\n   Press Ctrl+C or stop the cell to end the sweep early.\")\n",
    "print(f\"   Progress will be saved and you can resume later.\\n\")\n",
    "\n",
    "wandb.agent(\n",
    "    sweep_id=sweep_id,\n",
    "    function=sweep_train,\n",
    "    count=SWEEP_CONFIG['max_runs'],\n",
    "    project=SWEEP_CONFIG['wandb_project'],\n",
    "    entity=SWEEP_CONFIG['wandb_entity']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Resume or Join Existing Sweep\n",
    "\n",
    "Use this section to continue a previous sweep or join an existing one from another machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUME/JOIN EXISTING SWEEP\n",
    "# ============================================================\n",
    "\n",
    "# Set this to your existing sweep ID to resume\n",
    "EXISTING_SWEEP_ID = None  # e.g., 'abc123xy' or 'entity/project/abc123xy'\n",
    "\n",
    "if EXISTING_SWEEP_ID:\n",
    "    print(f\"üîÑ Resuming sweep: {EXISTING_SWEEP_ID}\")\n",
    "    print(f\"   Running {SWEEP_CONFIG['max_runs']} more runs...\\n\")\n",
    "    \n",
    "    wandb.agent(\n",
    "        sweep_id=EXISTING_SWEEP_ID,\n",
    "        function=sweep_train,\n",
    "        count=SWEEP_CONFIG['max_runs'],\n",
    "        project=SWEEP_CONFIG['wandb_project'],\n",
    "        entity=SWEEP_CONFIG['wandb_entity']\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Set EXISTING_SWEEP_ID to resume a previous sweep.\")\n",
    "    print(\"   Example: EXISTING_SWEEP_ID = 'abc123xy'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Sweep Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANALYZE SWEEP RESULTS\n",
    "# ============================================================\n",
    "\n",
    "def get_sweep_results(sweep_id, project='cubemaster', entity=None):\n",
    "    \"\"\"Fetch and analyze sweep results from wandb.\"\"\"\n",
    "    api = wandb.Api()\n",
    "    \n",
    "    # Get sweep\n",
    "    sweep_path = f\"{entity}/{project}/{sweep_id}\" if entity else f\"{project}/{sweep_id}\"\n",
    "    sweep = api.sweep(sweep_path)\n",
    "    \n",
    "    print(f\"üìä Sweep: {sweep.name}\")\n",
    "    print(f\"   State: {sweep.state}\")\n",
    "    print(f\"   Runs: {len(sweep.runs)}\")\n",
    "    \n",
    "    # Collect run data\n",
    "    runs_data = []\n",
    "    for run in sweep.runs:\n",
    "        if run.state == 'finished':\n",
    "            summary = run.summary._json_dict\n",
    "            config = {k: v for k, v in run.config.items() if not k.startswith('_')}\n",
    "            runs_data.append({\n",
    "                'name': run.name,\n",
    "                'val_acc': summary.get('val_acc', 0),\n",
    "                'train_acc': summary.get('train_acc', 0),\n",
    "                'val_loss': summary.get('val_loss', float('inf')),\n",
    "                **config\n",
    "            })\n",
    "    \n",
    "    if runs_data:\n",
    "        # Sort by validation accuracy\n",
    "        runs_data.sort(key=lambda x: x['val_acc'], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüèÜ Top 5 Runs:\")\n",
    "        for i, run in enumerate(runs_data[:5], 1):\n",
    "            print(f\"   {i}. {run['name']}: {run['val_acc']:.2f}% val_acc\")\n",
    "            print(f\"      lr={run.get('lr', 'N/A'):.6f}, batch={run.get('batch_size', 'N/A')}, dropout={run.get('dropout_rate', 'N/A'):.2f}\")\n",
    "        \n",
    "        # Best config\n",
    "        best = runs_data[0]\n",
    "        print(f\"\\nüéØ Best Configuration:\")\n",
    "        for key, value in best.items():\n",
    "            if key not in ['name', 'val_acc', 'train_acc', 'val_loss']:\n",
    "                print(f\"   {key}: {value}\")\n",
    "        \n",
    "        return runs_data\n",
    "    else:\n",
    "        print(\"   No finished runs yet.\")\n",
    "        return []\n",
    "\n",
    "# Analyze the current sweep (uncomment after running sweep)\n",
    "# results = get_sweep_results(sweep_id, project=SWEEP_CONFIG['wandb_project'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN BEST CONFIG FROM SWEEP\n",
    "# ============================================================\n",
    "\n",
    "def train_best_from_sweep(sweep_id, project='cubemaster', entity=None, epochs=50):\n",
    "    \"\"\"Train a model with the best hyperparameters from a sweep.\"\"\"\n",
    "    api = wandb.Api()\n",
    "    sweep_path = f\"{entity}/{project}/{sweep_id}\" if entity else f\"{project}/{sweep_id}\"\n",
    "    sweep = api.sweep(sweep_path)\n",
    "    \n",
    "    # Find best run\n",
    "    best_run = sweep.best_run()\n",
    "    if not best_run:\n",
    "        print(\"‚ùå No completed runs found in sweep.\")\n",
    "        return None\n",
    "    \n",
    "    best_config = {k: v for k, v in best_run.config.items() if not k.startswith('_')}\n",
    "    \n",
    "    print(f\"üèÜ Training with best config from: {best_run.name}\")\n",
    "    print(f\"   Original val_acc: {best_run.summary.get('val_acc', 0):.2f}%\")\n",
    "    print(f\"   Config: {best_config}\")\n",
    "    print(f\"   Training for {epochs} epochs...\\n\")\n",
    "    \n",
    "    # Initialize new run\n",
    "    run = wandb.init(\n",
    "        project=project,\n",
    "        name=f\"{SWEEP_MODEL}_best_config\",\n",
    "        config=best_config,\n",
    "        tags=['best_from_sweep', SWEEP_MODEL]\n",
    "    )\n",
    "    \n",
    "    # Override epochs for longer training\n",
    "    original_epochs = SWEEP_CONFIG['epochs_per_run']\n",
    "    SWEEP_CONFIG['epochs_per_run'] = epochs\n",
    "    \n",
    "    try:\n",
    "        sweep_train()\n",
    "    finally:\n",
    "        SWEEP_CONFIG['epochs_per_run'] = original_epochs\n",
    "        wandb.finish()\n",
    "    \n",
    "    return run\n",
    "\n",
    "# Train with best config (uncomment after analyzing sweep)\n",
    "# train_best_from_sweep(sweep_id, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Colab-Specific Tips\n",
    "\n",
    "**Session Management:**\n",
    "- Colab sessions timeout after ~90 minutes of inactivity\n",
    "- Free tier has ~12 hour runtime limit\n",
    "- Keep the browser tab active or use browser extensions to prevent timeout\n",
    "\n",
    "**Sweep Strategy for Colab:**\n",
    "1. Run sweeps with `max_runs=20-30` to stay within session limits\n",
    "2. Save your `sweep_id` and resume later if needed\n",
    "3. Use shorter `epochs_per_run` (10-20) for initial exploration\n",
    "4. Train best config with more epochs after sweep completes\n",
    "\n",
    "**Multi-Session Sweeps:**\n",
    "```python\n",
    "# Session 1: Create sweep\n",
    "sweep_id = wandb.sweep(config, project='cubemaster')\n",
    "print(f\"Save this ID: {sweep_id}\")  # Copy this!\n",
    "\n",
    "# Session 2+: Resume sweep\n",
    "wandb.agent(sweep_id='YOUR_SWEEP_ID', function=sweep_train, count=20)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK SWEEP: Run a small sweep for testing\n",
    "# ============================================================\n",
    "\n",
    "# Minimal sweep config for quick testing\n",
    "QUICK_SWEEP_CONFIG = {\n",
    "    'method': 'random',\n",
    "    'metric': {'name': 'val_acc', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'lr': {'values': [0.001, 0.0005, 0.0001]},\n",
    "        'batch_size': {'values': [32, 64]},\n",
    "        'dropout_rate': {'values': [0.3, 0.5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_quick_sweep(num_runs=5, epochs=5):\n",
    "    \"\"\"Run a quick sweep for testing.\"\"\"\n",
    "    print(f\"‚ö° Running quick sweep: {num_runs} runs, {epochs} epochs each\")\n",
    "    \n",
    "    # Save original epochs\n",
    "    original_epochs = SWEEP_CONFIG['epochs_per_run']\n",
    "    SWEEP_CONFIG['epochs_per_run'] = epochs\n",
    "    \n",
    "    # Create and run sweep\n",
    "    quick_sweep_id = wandb.sweep(\n",
    "        sweep=QUICK_SWEEP_CONFIG,\n",
    "        project=SWEEP_CONFIG['wandb_project']\n",
    "    )\n",
    "    \n",
    "    print(f\"   Sweep ID: {quick_sweep_id}\")\n",
    "    \n",
    "    try:\n",
    "        wandb.agent(\n",
    "            sweep_id=quick_sweep_id,\n",
    "            function=sweep_train,\n",
    "            count=num_runs\n",
    "        )\n",
    "    finally:\n",
    "        SWEEP_CONFIG['epochs_per_run'] = original_epochs\n",
    "    \n",
    "    return quick_sweep_id\n",
    "\n",
    "# Uncomment to run a quick test sweep\n",
    "# quick_id = run_quick_sweep(num_runs=3, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results Visualization\n",
    "\n",
    "### 5.1 Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_training_curves(history, title=\"Training Curves\"):\n",
    "    \"\"\"Plot training and validation curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Loss Curves')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "    axes[1].plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Accuracy Curves')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark best epoch\n",
    "    best_epoch = np.argmax(history['val_acc']) + 1\n",
    "    best_acc = max(history['val_acc'])\n",
    "    axes[1].axvline(x=best_epoch, color='g', linestyle='--', alpha=0.5, label=f'Best: {best_acc:.1f}%')\n",
    "    axes[1].scatter([best_epoch], [best_acc], color='g', s=100, zorder=5)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot training curves\n",
    "if 'history' in dir():\n",
    "    plot_training_curves(history, f\"{TRAINING_CONFIG['model_type'].upper()} Training Curves\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No training history available. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from cubemaster import COLOR_CLASSES\n",
    "\n",
    "def evaluate_and_plot_confusion_matrix(model, test_loader, device):\n",
    "    \"\"\"Evaluate model and plot confusion matrix.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n",
    "    print(f\"\\nüìä Test Accuracy: {accuracy:.2f}%\\n\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=COLOR_CLASSES))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=COLOR_CLASSES, yticklabels=COLOR_CLASSES, ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(f'Confusion Matrix (Accuracy: {accuracy:.2f}%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, cm\n",
    "\n",
    "# Evaluate on test set\n",
    "if 'model' in dir() and 'test_loader' in dir():\n",
    "    device = get_device()\n",
    "    test_accuracy, cm = evaluate_and_plot_confusion_matrix(model, test_loader, device)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No model or test data available. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model Comparison (Optional)\n",
    "\n",
    "Train both models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(results_dict):\n",
    "    \"\"\"Compare multiple models side by side.\"\"\"\n",
    "    if len(results_dict) < 2:\n",
    "        print(\"‚ÑπÔ∏è Need at least 2 models to compare.\")\n",
    "        return\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä MODEL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Model':<20} {'Test Acc':>12} {'Parameters':>15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for name, data in results_dict.items():\n",
    "        acc = data.get('accuracy', 'N/A')\n",
    "        params = data.get('params', 'N/A')\n",
    "        print(f\"{name:<20} {acc:>11.2f}% {params:>15,}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    models = list(results_dict.keys())\n",
    "    accuracies = [results_dict[m]['accuracy'] for m in models]\n",
    "    \n",
    "    bars = ax.bar(models, accuracies, color=['#2196F3', '#4CAF50'][:len(models)])\n",
    "    ax.set_ylabel('Test Accuracy (%)')\n",
    "    ax.set_title('Model Comparison')\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{acc:.1f}%', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (uncomment after training both models)\n",
    "# comparison_results = {\n",
    "#     'MLP': {'accuracy': mlp_test_accuracy, 'params': mlp_model.count_parameters()['trainable']},\n",
    "#     'Shallow CNN': {'accuracy': cnn_test_accuracy, 'params': cnn_model.count_parameters()['trainable']}\n",
    "# }\n",
    "# compare_models(comparison_results)\n",
    "print(\"‚ÑπÔ∏è Train both MLP and Shallow CNN to enable comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Export\n",
    "\n",
    "### 6.1 Save PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def save_pytorch_model(model, trainer, model_name, output_dir='/content/exported_models'):\n",
    "    \"\"\"Save trained model in PyTorch format.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save full checkpoint (includes optimizer state, epoch, etc.)\n",
    "    checkpoint_path = output_dir / f\"{model_name}_checkpoint.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': model.get_config(),\n",
    "        'best_val_acc': trainer.best_val_acc,\n",
    "        'epoch': trainer.current_epoch,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"‚úÖ Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    # Save model weights only (smaller file)\n",
    "    weights_path = output_dir / f\"{model_name}_weights.pth\"\n",
    "    torch.save(model.state_dict(), weights_path)\n",
    "    print(f\"‚úÖ Weights saved: {weights_path}\")\n",
    "    \n",
    "    return checkpoint_path, weights_path\n",
    "\n",
    "# Save trained model\n",
    "if 'model' in dir() and 'trainer' in dir():\n",
    "    model_name = TRAINING_CONFIG['model_type']\n",
    "    checkpoint_path, weights_path = save_pytorch_model(model, trainer, model_name)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No model available. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Export to ONNX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "def export_to_onnx(model, model_name, output_dir='/content/exported_models', input_size=(50, 50)):\n",
    "    \"\"\"Export model to ONNX format.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    onnx_path = output_dir / f\"{model_name}.onnx\"\n",
    "    \n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(1, 3, input_size[0], input_size[1])\n",
    "    \n",
    "    # Export to ONNX\n",
    "    print(f\"\\nüì¶ Exporting to ONNX...\")\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        str(onnx_path),\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        },\n",
    "        dynamo=False  # Use legacy exporter for compatibility\n",
    "    )\n",
    "    \n",
    "    # Verify ONNX model\n",
    "    onnx_model = onnx.load(str(onnx_path))\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(f\"‚úÖ ONNX model exported and verified: {onnx_path}\")\n",
    "    \n",
    "    # Test with ONNX Runtime\n",
    "    ort_session = ort.InferenceSession(str(onnx_path))\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: dummy_input.numpy()}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    print(f\"   Output shape: {ort_outputs[0].shape}\")\n",
    "    \n",
    "    # Get file size\n",
    "    size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "    print(f\"   File size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    return onnx_path\n",
    "\n",
    "# Export to ONNX\n",
    "if 'model' in dir():\n",
    "    model_name = TRAINING_CONFIG['model_type']\n",
    "    onnx_path = export_to_onnx(model, model_name)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No model available. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Download Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "def download_models(output_dir='/content/exported_models'):\n",
    "    \"\"\"Download all exported models.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    if not output_dir.exists():\n",
    "        print(\"‚ùå No exported models found. Export models first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüì• Available models for download:\")\n",
    "    for f in output_dir.iterdir():\n",
    "        size_mb = os.path.getsize(f) / (1024 * 1024)\n",
    "        print(f\"   - {f.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    # Create zip archive\n",
    "    import shutil\n",
    "    zip_path = '/content/cubemaster_models.zip'\n",
    "    shutil.make_archive('/content/cubemaster_models', 'zip', output_dir)\n",
    "    print(f\"\\nüì¶ Created archive: {zip_path}\")\n",
    "    \n",
    "    # Download\n",
    "    print(\"\\n‚¨áÔ∏è Starting download...\")\n",
    "    files.download(zip_path)\n",
    "\n",
    "# Download models\n",
    "download_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Copy to Google Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_to_drive(source_dir='/content/exported_models', drive_dest='/content/drive/MyDrive/CubeMaster/trained_models'):\n",
    "    \"\"\"Copy exported models to Google Drive.\"\"\"\n",
    "    source_dir = Path(source_dir)\n",
    "    drive_dest = Path(drive_dest)\n",
    "    \n",
    "    if not source_dir.exists():\n",
    "        print(\"‚ùå No exported models found.\")\n",
    "        return\n",
    "    \n",
    "    # Create destination directory\n",
    "    drive_dest.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy files\n",
    "    print(f\"\\nüìÅ Copying models to Google Drive...\")\n",
    "    for f in source_dir.iterdir():\n",
    "        dest_file = drive_dest / f.name\n",
    "        shutil.copy2(f, dest_file)\n",
    "        print(f\"   ‚úÖ {f.name} -> {dest_file}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ All models copied to: {drive_dest}\")\n",
    "\n",
    "# Copy to Drive (uncomment to run)\n",
    "# copy_to_drive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**1. GPU Not Available**\n",
    "- Go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "- If GPU quota exceeded, try later or use CPU (slower)\n",
    "\n",
    "**2. Session Timeout**\n",
    "- Colab sessions timeout after ~90 minutes of inactivity\n",
    "- Enable wandb to save training progress\n",
    "- Save checkpoints to Google Drive periodically\n",
    "\n",
    "**3. Out of Memory**\n",
    "- Reduce batch size\n",
    "- Use a simpler model\n",
    "- Clear memory: `torch.cuda.empty_cache()`\n",
    "\n",
    "**4. Dataset Upload Issues**\n",
    "- Use Google Drive for large datasets (>100MB)\n",
    "- Compress dataset as ZIP before uploading\n",
    "- Check file paths after extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache.\"\"\"\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úÖ GPU memory cleared\")\n",
    "        print(f\"   Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"   Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "# Uncomment to clear memory\n",
    "# clear_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
