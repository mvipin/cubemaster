{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubeMaster: Color Classification Model Training\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mvipin/cubemaster/blob/main/notebooks/colab_training.ipynb)\n",
    "\n",
    "This notebook enables training of **MLP** and **Shallow CNN** models for Rubik's Cube color classification using Google Colab's GPU resources.\n",
    "\n",
    "## Contents\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [Data Preparation](#2-data-preparation)\n",
    "3. [Model Training](#3-model-training)\n",
    "4. [Weights & Biases Integration](#4-weights--biases-integration)\n",
    "5. [Results Visualization](#5-results-visualization)\n",
    "6. [Model Export](#6-model-export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slower.\")\n",
    "    print(\"   Go to Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision torchmetrics\n",
    "!pip install -q albumentations opencv-python-headless Pillow\n",
    "!pip install -q numpy pyyaml tqdm\n",
    "!pip install -q matplotlib seaborn scikit-learn\n",
    "!pip install -q onnx onnxruntime\n",
    "!pip install -q wandb\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Install Git LFS (required for dataset files)\n",
    "!apt-get install -qq git-lfs\n",
    "!git lfs install\n",
    "\n",
    "# Clone the repository\n",
    "REPO_URL = \"https://github.com/mvipin/cubemaster.git\"\n",
    "REPO_DIR = Path(\"/content/cubemaster\")\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "    print(f\"‚úÖ Repository cloned to {REPO_DIR}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Repository already exists at {REPO_DIR}\")\n",
    "    # Pull latest changes\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# Pull LFS files (dataset images)\n",
    "print(\"\\nüì• Pulling Git LFS files (dataset images)...\")\n",
    "!cd {REPO_DIR} && git lfs pull\n",
    "print(\"‚úÖ LFS files downloaded\")\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Setup Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to Python path\n",
    "SRC_PATH = Path(\"/content/cubemaster/src\")\n",
    "if str(SRC_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_PATH))\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from cubemaster.models import MLPClassifier, ShallowCNNClassifier, MODEL_REGISTRY\n",
    "    from cubemaster.utils.config import load_config, get_device, set_seed\n",
    "    from cubemaster.training.dataset import CubeColorDataset\n",
    "    from cubemaster.training.augmentations import get_train_transforms, get_val_transforms\n",
    "    from cubemaster.training.trainer import Trainer, EarlyStopping\n",
    "    from cubemaster import COLOR_CLASSES\n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "    print(f\"   Available models: {list(MODEL_REGISTRY.keys())}\")\n",
    "    print(f\"   Color classes: {COLOR_CLASSES}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"   Make sure the repository is cloned correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Preparation\n",
    "\n",
    "### 2.1 Dataset Selection\n",
    "\n",
    "The CubeMaster repository includes a pre-processed dataset with 4,410 images ready for training.\n",
    "\n",
    "**Dataset options:**\n",
    "1. **Repository Dataset** (default) - Use the included `data/processed/` dataset\n",
    "2. **Google Drive** - Upload your own dataset via Google Drive\n",
    "3. **Direct Upload** - Upload a zip file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Dataset source selection\n",
    "# Options: 'repository', 'google_drive', 'upload'\n",
    "DATASET_SOURCE = 'repository'  # Default: use included dataset\n",
    "\n",
    "if DATASET_SOURCE == 'repository':\n",
    "    # Use the dataset included in the cloned repository\n",
    "    DATASET_PATH = Path(\"/content/cubemaster/data/processed\")\n",
    "    \n",
    "    if DATASET_PATH.exists():\n",
    "        print(f\"‚úÖ Using repository dataset: {DATASET_PATH}\")\n",
    "        print(\"   This dataset contains 4,410 images (train: 4180, val: 101, test: 129)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Repository dataset not found at {DATASET_PATH}\")\n",
    "        print(\"   Make sure the repository was cloned correctly.\")\n",
    "\n",
    "elif DATASET_SOURCE == 'google_drive':\n",
    "    # Mount Google Drive and use custom dataset\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set path to your dataset in Google Drive\n",
    "    # Expected structure: dataset/train/{B,G,O,R,W,Y}/*.jpg\n",
    "    DATASET_PATH = Path(\"/content/drive/MyDrive/CubeMaster/dataset\")\n",
    "    print(f\"üìÅ Using Google Drive dataset: {DATASET_PATH}\")\n",
    "\n",
    "elif DATASET_SOURCE == 'upload':\n",
    "    # Direct upload via Colab file picker\n",
    "    from google.colab import files\n",
    "    import zipfile\n",
    "    \n",
    "    print(\"Upload your dataset as a zip file:\")\n",
    "    print(\"Expected structure: train/{B,G,O,R,W,Y}/*.jpg\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Extract uploaded zip\n",
    "    for filename in uploaded.keys():\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('/content/dataset')\n",
    "    DATASET_PATH = Path(\"/content/dataset\")\n",
    "    print(f\"‚úÖ Dataset extracted to {DATASET_PATH}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown DATASET_SOURCE: {DATASET_SOURCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Verify Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def verify_dataset(dataset_path: Path):\n",
    "    \"\"\"Verify dataset structure and count samples.\"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    if not dataset_path.exists():\n",
    "        print(f\"‚ùå Dataset path does not exist: {dataset_path}\")\n",
    "        return None\n",
    "    \n",
    "    splits = ['train', 'val', 'test']\n",
    "    expected_classes = ['B', 'G', 'O', 'R', 'W', 'Y']\n",
    "    stats = defaultdict(dict)\n",
    "    \n",
    "    print(f\"üìä Dataset Statistics for: {dataset_path}\\n\")\n",
    "    print(f\"{'Split':<10} {'B':>6} {'G':>6} {'O':>6} {'R':>6} {'W':>6} {'Y':>6} {'Total':>8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for split in splits:\n",
    "        split_path = dataset_path / split\n",
    "        if not split_path.exists():\n",
    "            print(f\"‚ö†Ô∏è  {split:<10} - Directory not found\")\n",
    "            continue\n",
    "        \n",
    "        total = 0\n",
    "        row = f\"{split:<10}\"\n",
    "        for cls in expected_classes:\n",
    "            cls_path = split_path / cls\n",
    "            if cls_path.exists():\n",
    "                count = len(list(cls_path.glob('*.png'))) + len(list(cls_path.glob('*.jpg')))\n",
    "                stats[split][cls] = count\n",
    "                total += count\n",
    "                row += f\" {count:>6}\"\n",
    "            else:\n",
    "                row += f\" {'N/A':>6}\"\n",
    "        stats[split]['total'] = total\n",
    "        row += f\" {total:>8}\"\n",
    "        print(row)\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    return stats\n",
    "\n",
    "# Verify dataset\n",
    "dataset_stats = verify_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Setup Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from cubemaster.training.dataset import CubeColorDataset\n",
    "from cubemaster.training.augmentations import get_train_transforms, get_val_transforms\n",
    "\n",
    "def create_data_loaders(dataset_path, batch_size=32, num_workers=2, image_size=(50, 50)):\n",
    "    \"\"\"Create train, validation, and test data loaders.\"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Get transforms\n",
    "    train_transform = get_train_transforms(image_size)\n",
    "    val_transform = get_val_transforms(image_size)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CubeColorDataset(\n",
    "        root_dir=dataset_path / 'train',\n",
    "        transform=train_transform\n",
    "    )\n",
    "    val_dataset = CubeColorDataset(\n",
    "        root_dir=dataset_path / 'val',\n",
    "        transform=val_transform\n",
    "    )\n",
    "    test_dataset = CubeColorDataset(\n",
    "        root_dir=dataset_path / 'test',\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data loaders created:\")\n",
    "    print(f\"   Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
    "    print(f\"   Val:   {len(val_dataset)} samples, {len(val_loader)} batches\")\n",
    "    print(f\"   Test:  {len(test_dataset)} samples, {len(test_loader)} batches\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Create data loaders (will be called after config is set)\n",
    "print(\"Data loader function defined. Will be created during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Training\n",
    "\n",
    "### 3.1 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from cubemaster.utils.config import load_config, set_seed\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    # Model selection: 'mlp' or 'shallow_cnn'\n",
    "    'model_type': 'mlp',  # Change to 'shallow_cnn' for CNN training\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'batch_size': 32,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    \n",
    "    # MLP-specific\n",
    "    'mlp_hidden_dims': [256, 128],\n",
    "    'mlp_dropout': 0.3,\n",
    "    \n",
    "    # Shallow CNN-specific\n",
    "    'cnn_dropout': 0.5,\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 10,\n",
    "    'min_delta': 0.001,\n",
    "    \n",
    "    # Reproducibility\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Wandb (optional)\n",
    "    'use_wandb': False,  # Set to True to enable wandb logging\n",
    "    'wandb_project': 'cubemaster',\n",
    "}\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(TRAINING_CONFIG['seed'])\n",
    "print(f\"‚úÖ Configuration set for {TRAINING_CONFIG['model_type'].upper()} training\")\n",
    "print(f\"   Epochs: {TRAINING_CONFIG['epochs']}, Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"   Learning rate: {TRAINING_CONFIG['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from cubemaster.models import MLPClassifier\n",
    "from cubemaster.training.trainer import Trainer, EarlyStopping\n",
    "from cubemaster.utils.config import get_device\n",
    "\n",
    "def train_mlp(config, train_loader, val_loader):\n",
    "    \"\"\"Train MLP model with given configuration.\"\"\"\n",
    "    device = get_device()\n",
    "    print(f\"\\nüöÄ Training MLP on {device}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create model\n",
    "    model = MLPClassifier(\n",
    "        num_classes=6,\n",
    "        input_size=(50, 50),\n",
    "        hidden_dims=config['mlp_hidden_dims'],\n",
    "        dropout_rate=config['mlp_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    params = model.count_parameters()\n",
    "    print(f\"Model parameters: {params['trainable']:,} trainable\")\n",
    "    \n",
    "    # Setup training components\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=config['patience'],\n",
    "        min_delta=config['min_delta']\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        early_stopping=early_stopping,\n",
    "        checkpoint_dir=Path('/content/cubemaster/models/mlp'),\n",
    "        use_wandb=config['use_wandb']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = trainer.fit(train_loader, val_loader, epochs=config['epochs'])\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Training complete! Best val accuracy: {trainer.best_val_acc:.2f}%\")\n",
    "    \n",
    "    return model, history, trainer\n",
    "\n",
    "# Train MLP (set model_type='mlp' in config above)\n",
    "if TRAINING_CONFIG['model_type'] == 'mlp':\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        DATASET_PATH, \n",
    "        batch_size=TRAINING_CONFIG['batch_size']\n",
    "    )\n",
    "    mlp_model, mlp_history, mlp_trainer = train_mlp(TRAINING_CONFIG, train_loader, val_loader)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Skipping MLP training. Set model_type='mlp' to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train Shallow CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cubemaster.models import ShallowCNNClassifier\n",
    "\n",
    "def train_shallow_cnn(config, train_loader, val_loader):\n",
    "    \"\"\"Train Shallow CNN model with given configuration.\"\"\"\n",
    "    device = get_device()\n",
    "    print(f\"\\nüöÄ Training Shallow CNN on {device}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create model\n",
    "    model = ShallowCNNClassifier(\n",
    "        num_classes=6,\n",
    "        input_size=(50, 50),\n",
    "        dropout_rate=config['cnn_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    params = model.count_parameters()\n",
    "    print(f\"Model parameters: {params['trainable']:,} trainable\")\n",
    "    \n",
    "    # Setup training components\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=config['patience'],\n",
    "        min_delta=config['min_delta']\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        early_stopping=early_stopping,\n",
    "        checkpoint_dir=Path('/content/cubemaster/models/shallow_cnn'),\n",
    "        use_wandb=config['use_wandb']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = trainer.fit(train_loader, val_loader, epochs=config['epochs'])\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Training complete! Best val accuracy: {trainer.best_val_acc:.2f}%\")\n",
    "    \n",
    "    return model, history, trainer\n",
    "\n",
    "# Train Shallow CNN (set model_type='shallow_cnn' in config above)\n",
    "if TRAINING_CONFIG['model_type'] == 'shallow_cnn':\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        DATASET_PATH, \n",
    "        batch_size=TRAINING_CONFIG['batch_size']\n",
    "    )\n",
    "    cnn_model, cnn_history, cnn_trainer = train_shallow_cnn(TRAINING_CONFIG, train_loader, val_loader)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Skipping Shallow CNN training. Set model_type='shallow_cnn' to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Weights & Biases Integration\n",
    "\n",
    "### 4.1 Wandb Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb setup for Colab\n",
    "import wandb\n",
    "\n",
    "# Login to wandb (will prompt for API key)\n",
    "# Get your API key from: https://wandb.ai/authorize\n",
    "wandb.login()\n",
    "\n",
    "print(\"‚úÖ Logged into Weights & Biases!\")\n",
    "print(\"   Dashboard: https://wandb.ai/home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train with Wandb Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable wandb and retrain\n",
    "TRAINING_CONFIG['use_wandb'] = True\n",
    "\n",
    "# Initialize wandb run\n",
    "run = wandb.init(\n",
    "    project=TRAINING_CONFIG['wandb_project'],\n",
    "    name=f\"{TRAINING_CONFIG['model_type']}_colab_{TRAINING_CONFIG['seed']}\",\n",
    "    config=TRAINING_CONFIG,\n",
    "    tags=['colab', TRAINING_CONFIG['model_type']]\n",
    ")\n",
    "\n",
    "print(f\"üìä Wandb run initialized: {run.url}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    DATASET_PATH, \n",
    "    batch_size=TRAINING_CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "# Train selected model with wandb\n",
    "if TRAINING_CONFIG['model_type'] == 'mlp':\n",
    "    model, history, trainer = train_mlp(TRAINING_CONFIG, train_loader, val_loader)\n",
    "else:\n",
    "    model, history, trainer = train_shallow_cnn(TRAINING_CONFIG, train_loader, val_loader)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "print(f\"\\n‚úÖ Training logged to wandb: {run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results Visualization\n",
    "\n",
    "### 5.1 Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_training_curves(history, title=\"Training Curves\"):\n",
    "    \"\"\"Plot training and validation curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Loss Curves')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "    axes[1].plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Accuracy Curves')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark best epoch\n",
    "    best_epoch = np.argmax(history['val_acc']) + 1\n",
    "    best_acc = max(history['val_acc'])\n",
    "    axes[1].axvline(x=best_epoch, color='g', linestyle='--', alpha=0.5, label=f'Best: {best_acc:.1f}%')\n",
    "    axes[1].scatter([best_epoch], [best_acc], color='g', s=100, zorder=5)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot training curves\n",
    "if 'history' in dir():\n",
    "    plot_training_curves(history, f\"{TRAINING_CONFIG['model_type'].upper()} Training Curves\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No training history available. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from cubemaster import COLOR_CLASSES\n",
    "\n",
    "def evaluate_and_plot_confusion_matrix(model, test_loader, device):\n",
    "    \"\"\"Evaluate model and plot confusion matrix.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n",
    "    print(f\"\\nüìä Test Accuracy: {accuracy:.2f}%\\n\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=COLOR_CLASSES))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=COLOR_CLASSES, yticklabels=COLOR_CLASSES, ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(f'Confusion Matrix (Accuracy: {accuracy:.2f}%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, cm\n",
    "\n",
    "# Evaluate on test set\n",
    "if 'model' in dir() and 'test_loader' in dir():\n",
    "    device = get_device()\n",
    "    test_accuracy, cm = evaluate_and_plot_confusion_matrix(model, test_loader, device)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No model or test data available. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model Comparison (Optional)\n",
    "\n",
    "Train both models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(results_dict):\n",
    "    \"\"\"Compare multiple models side by side.\"\"\"\n",
    "    if len(results_dict) < 2:\n",
    "        print(\"‚ÑπÔ∏è Need at least 2 models to compare.\")\n",
    "        return\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä MODEL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Model':<20} {'Test Acc':>12} {'Parameters':>15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for name, data in results_dict.items():\n",
    "        acc = data.get('accuracy', 'N/A')\n",
    "        params = data.get('params', 'N/A')\n",
    "        print(f\"{name:<20} {acc:>11.2f}% {params:>15,}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    models = list(results_dict.keys())\n",
    "    accuracies = [results_dict[m]['accuracy'] for m in models]\n",
    "    \n",
    "    bars = ax.bar(models, accuracies, color=['#2196F3', '#4CAF50'][:len(models)])\n",
    "    ax.set_ylabel('Test Accuracy (%)')\n",
    "    ax.set_title('Model Comparison')\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{acc:.1f}%', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (uncomment after training both models)\n",
    "# comparison_results = {\n",
    "#     'MLP': {'accuracy': mlp_test_accuracy, 'params': mlp_model.count_parameters()['trainable']},\n",
    "#     'Shallow CNN': {'accuracy': cnn_test_accuracy, 'params': cnn_model.count_parameters()['trainable']}\n",
    "# }\n",
    "# compare_models(comparison_results)\n",
    "print(\"‚ÑπÔ∏è Train both MLP and Shallow CNN to enable comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Export\n",
    "\n",
    "### 6.1 Save PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def save_pytorch_model(model, trainer, model_name, output_dir='/content/exported_models'):\n",
    "    \"\"\"Save trained model in PyTorch format.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save full checkpoint (includes optimizer state, epoch, etc.)\n",
    "    checkpoint_path = output_dir / f\"{model_name}_checkpoint.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': model.get_config(),\n",
    "        'best_val_acc': trainer.best_val_acc,\n",
    "        'epoch': trainer.current_epoch,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"‚úÖ Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    # Save model weights only (smaller file)\n",
    "    weights_path = output_dir / f\"{model_name}_weights.pth\"\n",
    "    torch.save(model.state_dict(), weights_path)\n",
    "    print(f\"‚úÖ Weights saved: {weights_path}\")\n",
    "    \n",
    "    return checkpoint_path, weights_path\n",
    "\n",
    "# Save trained model\n",
    "if 'model' in dir() and 'trainer' in dir():\n",
    "    model_name = TRAINING_CONFIG['model_type']\n",
    "    checkpoint_path, weights_path = save_pytorch_model(model, trainer, model_name)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No model available. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Export to ONNX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "def export_to_onnx(model, model_name, output_dir='/content/exported_models', input_size=(50, 50)):\n",
    "    \"\"\"Export model to ONNX format.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    onnx_path = output_dir / f\"{model_name}.onnx\"\n",
    "    \n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(1, 3, input_size[0], input_size[1])\n",
    "    \n",
    "    # Export to ONNX\n",
    "    print(f\"\\nüì¶ Exporting to ONNX...\")\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        str(onnx_path),\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        },\n",
    "        dynamo=False  # Use legacy exporter for compatibility\n",
    "    )\n",
    "    \n",
    "    # Verify ONNX model\n",
    "    onnx_model = onnx.load(str(onnx_path))\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(f\"‚úÖ ONNX model exported and verified: {onnx_path}\")\n",
    "    \n",
    "    # Test with ONNX Runtime\n",
    "    ort_session = ort.InferenceSession(str(onnx_path))\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: dummy_input.numpy()}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    print(f\"   Output shape: {ort_outputs[0].shape}\")\n",
    "    \n",
    "    # Get file size\n",
    "    size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "    print(f\"   File size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    return onnx_path\n",
    "\n",
    "# Export to ONNX\n",
    "if 'model' in dir():\n",
    "    model_name = TRAINING_CONFIG['model_type']\n",
    "    onnx_path = export_to_onnx(model, model_name)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No model available. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Download Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "def download_models(output_dir='/content/exported_models'):\n",
    "    \"\"\"Download all exported models.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    if not output_dir.exists():\n",
    "        print(\"‚ùå No exported models found. Export models first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüì• Available models for download:\")\n",
    "    for f in output_dir.iterdir():\n",
    "        size_mb = os.path.getsize(f) / (1024 * 1024)\n",
    "        print(f\"   - {f.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    # Create zip archive\n",
    "    import shutil\n",
    "    zip_path = '/content/cubemaster_models.zip'\n",
    "    shutil.make_archive('/content/cubemaster_models', 'zip', output_dir)\n",
    "    print(f\"\\nüì¶ Created archive: {zip_path}\")\n",
    "    \n",
    "    # Download\n",
    "    print(\"\\n‚¨áÔ∏è Starting download...\")\n",
    "    files.download(zip_path)\n",
    "\n",
    "# Download models\n",
    "download_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Copy to Google Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_to_drive(source_dir='/content/exported_models', drive_dest='/content/drive/MyDrive/CubeMaster/trained_models'):\n",
    "    \"\"\"Copy exported models to Google Drive.\"\"\"\n",
    "    source_dir = Path(source_dir)\n",
    "    drive_dest = Path(drive_dest)\n",
    "    \n",
    "    if not source_dir.exists():\n",
    "        print(\"‚ùå No exported models found.\")\n",
    "        return\n",
    "    \n",
    "    # Create destination directory\n",
    "    drive_dest.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy files\n",
    "    print(f\"\\nüìÅ Copying models to Google Drive...\")\n",
    "    for f in source_dir.iterdir():\n",
    "        dest_file = drive_dest / f.name\n",
    "        shutil.copy2(f, dest_file)\n",
    "        print(f\"   ‚úÖ {f.name} -> {dest_file}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ All models copied to: {drive_dest}\")\n",
    "\n",
    "# Copy to Drive (uncomment to run)\n",
    "# copy_to_drive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**1. GPU Not Available**\n",
    "- Go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "- If GPU quota exceeded, try later or use CPU (slower)\n",
    "\n",
    "**2. Session Timeout**\n",
    "- Colab sessions timeout after ~90 minutes of inactivity\n",
    "- Enable wandb to save training progress\n",
    "- Save checkpoints to Google Drive periodically\n",
    "\n",
    "**3. Out of Memory**\n",
    "- Reduce batch size\n",
    "- Use a simpler model\n",
    "- Clear memory: `torch.cuda.empty_cache()`\n",
    "\n",
    "**4. Dataset Upload Issues**\n",
    "- Use Google Drive for large datasets (>100MB)\n",
    "- Compress dataset as ZIP before uploading\n",
    "- Check file paths after extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache.\"\"\"\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úÖ GPU memory cleared\")\n",
    "        print(f\"   Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"   Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "# Uncomment to clear memory\n",
    "# clear_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
